{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dbbdb56-93d2-4687-8a17-d53b0036397a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# This is simple one time solution for this task but for ongoing basis I would like to keep the watermark information in control table also the file details to which it belong to so that I can track the data flow. I would make this as incremental load and read only the new files which are not processed. Also I would split these files in 2 tasks so that I can process the new files in parallel, and we do not need to process all files if one files fails for some reason. Also I would add some logging information like rows read/write, error logs etc.\n",
    "###########################\n",
    "\n",
    "df_claims = spark.read.csv(\"/Volumes/workspace/default/csv_files/data_engineer_exam_claims_final.csv\", header=True, inferSchema=True)\n",
    "df_rx = spark.read.csv(\"/Volumes/workspace/default/csv_files/data_engineer_exam_rx_final.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d63d1fad-4cb6-4ad2-9728-ca18f245870f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# These managed tables would be External delta tables and will be incremental load where we will define the primary key fields and upsert the data as Delta tables keep track of the data changes.\n",
    "############################\n",
    "df_claims.write.mode(\"overwrite\").saveAsTable(\"claims\")\n",
    "df_rx.write.mode(\"overwrite\").saveAsTable(\"rx\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load_claims_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}